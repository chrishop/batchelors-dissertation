%-----------------------------------------------------------------------------------------
\clearpage
\section{Background Research}
%-----------------------------------------------------------------------------------------
\subsection{A Brief History}
One of the first pieces of research into Distributed Machine learning was
’Distributed Inference for Latent Dirichlet Allocation’ in 2008
\cite{newman2008distributed} One of the first instances of DistributedMachine
Learning  was  used  to  categorise  New York  Times  articles  using  Latent
Dirichlet Allocation (LDA), which identifies the affiliations words have to
certain topics.  While the paper focused on parallelising the algorithm and
running them over multiple artificially isolated cores the results showed that
distributed machine learning could have scalability and didn’t impact the rate
of convergence of the model.  This was followed by a paper by Jia et al.
\cite{ParallelTopicModels} which produced much faster results than its
predecessors by using memcache layer in every machine, every machine would
message every other machine with updates of its local parameters to create an
approximate global state, it was mentioned in passing that arranging the nodes
in a star topology and caching the values that passed through it could make the
system more scalable.  Then in 2014 came the parameter server as it is known
today \cite{LI2014ParameterServers}.They improved upon the shared memcache layer
by turning it into its own standalone server.