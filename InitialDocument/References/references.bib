@inproceedings{patel2011joomla,
  title={Joomla, Drupal and WordPress-a statistical comparison of open source CMS},
  author={Patel, Savan K and Rathod, VR and Parikh, Satyen},
  booktitle={Trendz in Information Sciences and Computing (TISC), 2011 3rd International Conference on},
  pages={182--187},
  year={2011},
  organization={IEEE}
},
@article{googlemapreduce2008,
author = {Dean, Jeffrey and Ghemawat, Sanjay},
title = {MapReduce: Simplified Data Processing on Large Clusters},
year = {2008},
issue_date = {January 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/1327452.1327492},
doi = {10.1145/1327452.1327492},
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
journal = {Commun. ACM},
month = jan,
pages = {107–113},
numpages = {7}
},
@misc{piergiovanni2020avid,
      title={AViD Dataset: Anonymized Videos from Diverse Countries}, 
      author={AJ Piergiovanni and Michael S. Ryoo},
      year={2020},
      eprint={2007.05515},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
},
@article{ParallelTopicModels,
author = {Smola, Alexander and Narayanamurthy, Shravan},
title = {An Architecture for Parallel Topic Models},
year = {2010},
issue_date = {September 2010},
publisher = {VLDB Endowment},
volume = {3},
number = {1–2},
issn = {2150-8097},
url = {https://doi.org/10.14778/1920841.1920931},
doi = {10.14778/1920841.1920931},
abstract = {This paper describes a high performance sampling architecture for inference of latent topic models on a cluster of workstations. Our system is faster than previous work by over an order of magnitude and it is capable of dealing with hundreds of millions of documents and thousands of topics.The algorithm relies on a novel communication structure, namely the use of a distributed (key, value) storage for synchronizing the sampler state between computers. Our architecture entirely obviates the need for separate computation and synchronization phases. Instead, disk, CPU, and network are used simultaneously to achieve high performance. We show that this architecture is entirely general and that it can be extended easily to more sophisticated latent variable models such as n-grams and hierarchies.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {703–710},
numpages = {8}
},
@inproceedings {LI2014ParameterServers,
author = {Mu Li and David G. Andersen and Jun Woo Park and Alexander J. Smola and Amr Ahmed and Vanja Josifovski and James Long and Eugene J. Shekita and Bor-Yiing Su},
title = {Scaling Distributed Machine Learning with the Parameter Server},
booktitle = {11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14)},
year = {2014},
isbn = { 978-1-931971-16-4},
address = {Broomfield, CO},
pages = {583--598},
url = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/li_mu},
publisher = {{USENIX} Association},
month = oct,
},
@Article{Mammograms2020,
author={McKinney, Scott Mayer
and Sieniek, Marcin
and Godbole, Varun
and Godwin, Jonathan
and Antropova, Natasha
and Ashrafian, Hutan
and Back, Trevor
and Chesus, Mary
and Corrado, Greg S.
and Darzi, Ara
and Etemadi, Mozziyar
and Garcia-Vicente, Florencia
and Gilbert, Fiona J.
and Halling-Brown, Mark
and Hassabis, Demis
and Jansen, Sunny
and Karthikesalingam, Alan
and Kelly, Christopher J.
and King, Dominic
and Ledsam, Joseph R.
and Melnick, David
and Mostofi, Hormuz
and Peng, Lily
and Reicher, Joshua Jay
and Romera-Paredes, Bernardino
and Sidebottom, Richard
and Suleyman, Mustafa
and Tse, Daniel
and Young, Kenneth C.
and De Fauw, Jeffrey
and Shetty, Shravya},
title={International evaluation of an AI system for breast cancer screening},
journal={Nature},
year={2020},
month={Jan},
day={01},
volume={577},
number={7788},
pages={89-94},
abstract={Screening mammography aims to identify breast cancer at earlier stages of the disease, when treatment can be more successful1. Despite the existence of screening programmes worldwide, the interpretation of mammograms is affected by high rates of false positives and false negatives2. Here we present an artificial intelligence (AI) system that is capable of surpassing human experts in breast cancer prediction. To assess its performance in the clinical setting, we curated a large representative dataset from the UK and a large enriched dataset from the USA. We show an absolute reduction of 5.7{\%} and 1.2{\%} (USA and UK) in false positives and 9.4{\%} and 2.7{\%} in false negatives. We provide evidence of the ability of the system to generalize from the UK to the USA. In an independent study of six radiologists, the AI system outperformed all of the human readers: the area under the receiver operating characteristic curve (AUC-ROC) for the AI system was greater than the AUC-ROC for the average radiologist by an absolute margin of 11.5{\%}. We ran a simulation in which the AI system participated in the double-reading process that is used in the UK, and found that the AI system maintained non-inferior performance and reduced the workload of the second reader by 88{\%}. This robust assessment of the AI system paves the way for clinical trials to improve the accuracy and efficiency of breast cancer screening.},
issn={1476-4687},
doi={10.1038/s41586-019-1799-6},
url={https://doi.org/10.1038/s41586-019-1799-6}
},
@misc{fewshowlearners2020gpt,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
},
@misc{rajbhandari2020zero,
      title={ZeRO: Memory Optimizations Toward Training Trillion Parameter Models}, 
      author={Samyam Rajbhandari and Jeff Rasley and Olatunji Ruwase and Yuxiong He},
      year={2020},
      eprint={1910.02054},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
},
@incollection{Qirong2013SSP,
title = {More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server},
author = {Ho, Qirong and Cipar, James and Cui, Henggang and Lee, Seunghak and Kim, Jin Kyu and Gibbons, Phillip B. and Gibson, Garth A and Ganger, Greg and Xing, Eric P},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {1223--1231},
year = {2013},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4894-more-effective-distributed-ml-via-a-stale-synchronous-parallel-parameter-server.pdf}
},
@ARTICLE{Xing2015Petuum,
author={E. P. {Xing} and Q. {Ho} and W. {Dai} and J. K. {Kim} and J. {Wei} and S. {Lee} and X. {Zheng} and P. {Xie} and A. {Kumar} and Y. {Yu}},
journal={IEEE Transactions on Big Data}, 
title={Petuum: A New Platform for Distributed Machine Learning on Big Data}, 
year={2015},
volume={1},
number={2},
pages={49-67}
},
@incollection{Dean2012Distbelief,
title = {Large Scale Distributed Deep Networks},
author = {Jeffrey Dean and Greg Corrado and Rajat Monga and Chen, Kai and Matthieu Devin and Mark Mao and Marc\textquotesingle aurelio Ranzato and Andrew Senior and Paul Tucker and Ke Yang and Quoc V. Le and Andrew Y. Ng},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1223--1231},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf}
},
@inproceedings{kim2016STRADS,
author = {Kim, Jin Kyu and Ho, Qirong and Lee, Seunghak and Zheng, Xun and Dai, Wei and Gibson, Garth A. and Xing, Eric P.},
title = {STRADS: A Distributed Framework for Scheduled Model Parallel Machine Learning},
year = {2016},
isbn = {9781450342407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901318.2901331},
doi = {10.1145/2901318.2901331},
abstract = {Machine learning (ML) algorithms are commonly applied to big data, using distributed systems that partition the data across machines and allow each machine to read and update all ML model parameters --- a strategy known as data parallelism. An alternative and complimentary strategy, model parallelism, partitions the model parameters for non-shared parallel access and updates, and may periodically repartition the parameters to facilitate communication. Model parallelism is motivated by two challenges that data-parallelism does not usually address: (1) parameters may be dependent, thus naive concurrent updates can introduce errors that slow convergence or even cause algorithm failure; (2) model parameters converge at different rates, thus a small subset of parameters can bottleneck ML algorithm completion. We propose scheduled model parallelism (SchMP), a programming approach that improves ML algorithm convergence speed by efficiently scheduling parameter updates, taking into account parameter dependencies and uneven convergence. To support SchMP at scale, we develop a distributed framework STRADS which optimizes the throughput of SchMP programs, and benchmark four common ML applications written as SchMP programs: LDA topic modeling, matrix factorization, sparse least-squares (Lasso) regression and sparse logistic regression. By improving ML progress per iteration through SchMP programming whilst improving iteration throughput through STRADS we show that SchMP programs running on STRADS outperform non-model-parallel ML implementations: for example, SchMP LDA and SchMP Lasso respectively achieve 10x and 5x faster convergence than recent, well-established baselines.},
booktitle = {Proceedings of the Eleventh European Conference on Computer Systems},
articleno = {5},
numpages = {16},
location = {London, United Kingdom},
series = {EuroSys '16}
},
@inproceedings {Chilimbi2014ADAM,
author = {Trishul Chilimbi and Yutaka Suzue and Johnson Apacible and Karthik Kalyanaraman},
title = {Project Adam: Building an Efficient and Scalable Deep Learning Training System},
booktitle = {11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14)},
year = {2014},
isbn = { 978-1-931971-16-4},
address = {Broomfield, CO},
pages = {571--582},
url = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/chilimbi},
publisher = {{USENIX} Association},
month = oct,
},
@article{Verbraeken2020MLSurvey,
author = {Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S.},
title = {A Survey on Distributed Machine Learning},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3377454},
doi = {10.1145/3377454},
abstract = {The demand for artificial intelligence has grown significantly over the past decade, and this growth has been fueled by advances in machine learning techniques and the ability to leverage hardware acceleration. However, to increase the quality of predictions and render machine learning solutions feasible for more complex applications, a substantial amount of training data is required. Although small machine learning models can be trained with modest amounts of data, the input for training larger models such as neural networks grows exponentially with the number of parameters. Since the demand for processing training data has outpaced the increase in computation power of computing machinery, there is a need for distributing the machine learning workload across multiple machines, and turning the centralized into a distributed system. These distributed systems present new challenges: first and foremost, the efficient parallelization of the training process and the creation of a coherent model. This article provides an extensive overview of the current state-of-the-art in the field by outlining the challenges and opportunities of distributed machine learning over conventional (centralized) machine learning, discussing the techniques used for distributed machine learning, and providing an overview of the systems that are available.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {30},
numpages = {33},
keywords = {Distributed machine learning, distributed systems}
},
@misc{jia2018BeyondData,
      title={Beyond Data and Model Parallelism for Deep Neural Networks}, 
      author={Zhihao Jia and Matei Zaharia and Alex Aiken},
      year={2018},
      eprint={1807.05358},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
},
@article {Sudre2020LongCorona,
	author = {Sudre, Carole H. and Murray, Benjamin and Varsavsky, Thomas and Graham, Mark S. and Penfold, Rose S. and Bowyer, Ruth C. and Pujol, Joan Capdevila and Klaser, Kerstin and Antonelli, Michela and Canas, Liane S. and Molteni, Erika and Modat, Marc and Cardoso, M. Jorge and May, Anna and Ganesh, Sajaysurya and Davies, Richard and Nguyen, Long H and Drew, David A. and Astley, Christina M. and Joshi, Amit D. and Merino, Jordi and Tsereteli, Neli and Fall, Tove and Gomez, Maria F. and Duncan, Emma L. and Menni, Cristina and Williams, Frances M.K. and Franks, Paul W. and Chan, Andrew T. and Wolf, Jonathan and Ourselin, Sebastien and Spector, Tim and Steves, Claire J.},
	title = {Attributes and predictors of Long-COVID: analysis of COVID cases and their symptoms collected by the Covid Symptoms Study App},
	elocation-id = {2020.10.19.20214494},
	year = {2020},
	doi = {10.1101/2020.10.19.20214494},
	publisher = {Cold Spring Harbor Laboratory Press},
	URL = {https://www.medrxiv.org/content/early/2020/10/21/2020.10.19.20214494},
	eprint = {https://www.medrxiv.org/content/early/2020/10/21/2020.10.19.20214494.full.pdf},
	journal = {medRxiv}
},
@article{fowler2001agile,
  title={The agile manifesto},
  author={Fowler, Martin and Highsmith, Jim and others},
  journal={Software Development},
  volume={9},
  number={8},
  pages={28--35},
  year={2001},
  publisher={[San Francisco, CA: Miller Freeman, Inc., 1993-}
},
@book{beck2003test,
  title={Test-driven development: by example},
  author={Beck, Kent},
  year={2003},
  publisher={Addison-Wesley Professional}
}



