%-----------------------------------------------------------------------------------------

\section{Introduction}
%-----------------------------------------------------------------------------------------

\subsection{Motivation and Context}

Machine Learning has become an invisible but ubiquitous part of modern life, and
is being used in a plethora of fields and industries. The uses of this
technology range from dystopian facial recognition
\cite{mattBurgessFacialRecognision} to lifesaving diagnoses
\cite{Mammograms2020} and many more purposes besides. Machine Learning leverages
existing data to train Machine Learning models in order to perform a task or
find patterns, that previously only a human could. The key difference between
Machine Learning and conventional programs is that the data itself is used to
develop the model. Therefore the quality and quantity of the data can affect the
effectiveness of a machine learning model.
\par
As the amount and complexity of the data we are collecting increases so does the
size and complexity of the machine learning models we use to make sense of our
data. For example the internet archive as of 2020 contains over 70 petabytes of
data, while labeled datasets such as AViD have video data in the order of
terabytes. \cite{piergiovanni2020avid} We are now reaching a point where the
limiting factor of creating a machine learning model is not the data, but the
machine learning algorithm itself.
\par
This problem is two pronged. First machine learning models are getting very,
very large. For instance GPT-3 the largest Natural Language Model ever trained
contains 175 billion parameters. \cite{fewshowlearners2020gpt} And efforts are
being made to create models with trillions of parameters.
\cite{rajbhandari2020zero} We have reached the point where its no longer
possible to store some machine learning models on a single machine.
\par
The second problem is that training a machine learning model is increasingly
taking longer and longer. This is because we have more data and larger models,
but the algorithms used to train models have fundamentally stayed the same and
are inherently sequential and difficult to parallelise.


% The success in its widespread
% use has been in part to the increasing availability and collection of data.
% Machine Learning algorithms have the ability to leverage this data with little 



