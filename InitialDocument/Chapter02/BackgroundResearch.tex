%-----------------------------------------------------------------------------------------
\clearpage
\section{Background Research}
%-----------------------------------------------------------------------------------------
\subsection{Brief Introduction to Machine Learning, Neural Networks and Stochastic Gradient Descent}
To first understand Distributed Machine Learning you must first understand the
fundamentals of Machine Learning and Neural Networks. 

There are many machine learning methods some requiring training data
(supervised) some being able to find patterns in data without being given
solutions (unsupervised). \cite{alpaydin2020introduction} An example of an
supervised system may be predicting house prices, using multiple factors about
each house (market data, geographic area, square footage etc.) to come to a
conclusion about what the house could sell for. This would be trained using data
of previously sold houses to predict current ones. An example of an unsupervised
system could be identification of new plant species. This could be done by
taking as many features of a plant as possible, then apply a clustering
algorithm to see if there are two distinct clusters in the data. If there were
then that would suggest two different plant species. Neural Networks tend to
focus on supervised learning and use a form Gradient Descent called Stochastic
Gradient Descent.

Many Machine Learning algorithms use a cost function to measure how well or
badly they are solving a problem, these algorithms also use parameters which
internal variables of a machine learning model define how they are solving the
problem. If you map \(costFunction(x)\), where \(x\) is the model parameter, for
every \(x\) value. Then a graph will be produced, the lowest point on the graph
will be the global minimum. There may be other troughs higher than the global
minimum these are called local minimums. A global minimum represents the lowest
value of the cost function which indicates the parameter values produce the best
solution for your problem. Initial model parameters are often randomised,
meaning they may start at a high point on the cost function graph, the goal is
to get to the lowest point possible. To do this you must \textit{descend} down
the \textit{gradient} to a local minima, the algorithm that does this is called
gradient descent for that very reason. This often happens in little steps after
the observation of each piece of data. However it is computationally expensive
to step down the gradient after each example. It is more efficient to calculate
the average step of a randomised selection of data. This is know as Stochastic
Gradient Descent.

Neural Networks are structures that can perform multi-variable gradient descent
when provided with training data. Neural Networks are comprised of layers of
interconnected neurons in a lattice like structure. Each neuron holds parameter
information the adjusting of which through stochastic gradient descent leads to
the solving of a problem through reaching the local minimum of the cost
function.

\subsection{Limited History of Distributed Machine Learning}
One of the first pieces of research into Distributed Machine learning was
’Distributed Inference for Latent Dirichlet Allocation’ in 2008
\cite{newman2008distributed} One of the first instances of DistributedMachine
Learning  was  used  to  categorise  New York  Times  articles  using  Latent
Dirichlet Allocation (LDA), which identifies the affiliations words have to
certain topics.  While the paper focused on parallelising the algorithm and
running them over multiple artificially isolated cores the results showed that
distributed machine learning could have scalability and didn’t impact the rate
of convergence of the model.  This was followed by a paper by Jia et al.
\cite{ParallelTopicModels} which produced much faster results than its
predecessors by using memcache layer in every machine, every machine would
message every other machine with updates of its local parameters to create an
approximate global state, it was mentioned in passing that arranging the nodes
in a star topology and caching the values that passed through it could make the
system more scalable. After this followed a cambrian explosion of work in this
area \cite{Ahmed2012YahooLDA, li2014communication, Dean2012Distbelief,
googlemapreduce2008} culminating in 2014 when the parameter server as it is
known today \cite{LI2014ParameterServers} was produced. This parameter server is
highly sophisticated and flexible accommodating the difference in hardware
components while spending more on computation and less time waiting.

\subsection{Model and Data Parallelism}
When creating distributed machine learning models there two different methods
for distributing training, Model Parallelism and data parallelism. These two
methods are not mutually exclusive and can be used in conjunction with one
another, such as in Distbelief. \cite{Dean2012Distbelief}. Model Parallelism is
when model parameters are split between the nodes. As Data Parallelism is when
the data is split between the nodes. \cite{Xing2015Petuum} Often with model
parallelism the whole set of training data is passed through each node. While in
Data Parallelism its common for each node to hold the whole machine learning
model.

The key advantage of Model Parallelism is that models can be far larger as they
no longer have to sit on one machine. However this one great advantage comes
with some disadvantages. Some parameters may take more time to converge than
others, this means that at times some nodes may be idle, so the spread of
computation is not equal or efficient. \cite{Dean2012Distbelief} Because some
parameters converge at different rates a scheduler can be used. However this
requires more computational overhead and communication and reduces iteration
throughput. \cite{kim2016STRADS}

Data Parallelism has the benefit that data throughput can be very large, making
processing using this method very fast. However with more nodes the
communication overhead increases as the nodes must communicate the changes in
their model parameters to each other. [cite someone here] The nodes can
communicate with each other synchronously, but this means the computation is
only as fast as the slowest node. If the nodes communicate with each other
asynchronously then some of the calculations will be made on out of date model
parameters so training examples may be needlessly wasted.

\subsection{Low Power Hardware, IoT and Edge Computing}

% While historically much of the focus of distributed learning has been to
% increase computational power in order to train bigger and more accurate models,
% more recently there has been research into the utility of the distributed aspect
% of Distributed Machine Learning. With the rise of Internet of Things (IoT)
% devices and the established ubiquity of smart phones more data than ever is
% being produced. Soon this data generation with exceed the capacity of the
% internet, and that over 90\% of data will be stored and processed locally.
% \cite{Chaing2016FogIoT} This will mean that data processed through machine
% learning algorithms may also have to be processed locally. This is an issue for
% machine learning as we know it, as it would no longer be viable to send all the
% data to one location to process it. Moreover local machines have magnitudes less
% power than centralised servers. However we still want to train machine learning
% algorithms as fast as we are able. But now it is also important that they are
% efficient, as they will be running on low power potentially battery powered
% devices.

Historically Machine learning algorithms have been focused on high model
accuracy through large models and vast amounts of training data, energy
consumption and efficiency has rarely been taken into consideration. However
with the rise of Internet of Things (IoT) devices and the established ubiquity
of smart phones more data than ever is being produced. Soon this data generation
with exceed the capacity of the internet, and experts estimate that over 90\% of
data will be stored and processed locally. \cite{Chaing2016FogIoT} By extension
this means machine learning algorithms will have to be performed locally too.
This introduces some challenging issues. Modern machine learning algorithms
require vast computational power and large amounts of data. Local devices don't
have the capacity to hold large data sets or the power to compute machine
learning models in a viable amount of time, while many of them are also battery
powered so power consumption becomes another issue.
