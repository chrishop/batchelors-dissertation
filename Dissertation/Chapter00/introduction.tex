%-----------------------------------------------------------------------------------------

\section{Introduction}
%-----------------------------------------------------------------------------------------

\subsection{Motivation and Context}

Machine Learning has become an invisible but ubiquitous part of modern life, and
is being used in a plethora of fields and industries. The uses of this
technology range from dystopian facial recognition
\cite{mattBurgessFacialRecognision} to lifesaving diagnoses
\cite{Mammograms2020} and many more purposes besides. machine learning leverages
existing data to train models in order to perform a task or find patterns that
previously only a human could. The key difference between machine learning and
conventional statistical analysis models is that the data itself is used to
develop it. Therefore the quality and quantity of the data can affect the
effectiveness of a machine learning model.
\par
As the amount and complexity of the data we are collecting increases, so does the
size and complexity of the machine learning models we use to make sense of our
data. For example the Internet Archive as of 2020 contains over 70 petabytes of
data, while labeled datasets such as AViD have video data in the order of
terabytes. \cite{piergiovanni2020avid} We are now reaching a point where the
limiting factor of creating a machine learning model is not the data, but the
machine learning algorithm itself.
\par
The problem is this. Machine learning models are getting very, very large. For
instance GPT-3 the largest NLP model ever trained contains 175 billion
parameters. \cite{fewshowlearners2020gpt} And efforts are being made to create
models with trillions of parameters. \cite{rajbhandari2020zero} We have reached
the point where its no longer viable to train some machine learning models on a
single machine.\cite{LI2014ParameterServers} This is because we have more data
and larger models, but the algorithms used to train these are inherently
sequential and difficult to parallelise.
\par
The popular current solution is to use a parameter server model. In brief the
paradigm is made of two different types of components, the parameter server and
the workers. The parameter server holds the global parameters of the model.
Workers each hold a separate set of training data and are given the model
parameters by the server. The workers then use an iterative algorithm to modify
the parameters. The modified parameters are then sent to the parameter server
where they are aggregated, the global model parameters are updated and the cycle
continues until the model has converged on an answer.

While this method has many benefits and is certainly faster than training using
a single machine, it has two key limiting factors. First, every worker must
communicate with a single parameter server, this limits scalability as
eventually the network bandwidth becomes saturated severely impacting
performance. \cite{LI2014ParameterServers} Secondly many parameter server models
require the whole model to be replicated within each node.
\cite{jia2018BeyondData} This means that all of the workers must be machines of
similar specification, else the higher performance machines will spend too much
time idle.
\par
In this paper I will outline an alternative distributed machine learning
framework: \textit{RingTMP}. RingTMP is a Ring Topological Model Parallel
distributed machine learning framework focusing on optimising distributed
gradient descent. This is a novel design drawing in inspiration much research
but particularly from the STRADS and DistBelief machine learning frameworks.
\cite{kim2016STRADS,Dean2012Distbelief} I believe my distributed framework may
have some advantages over the current paradigm, these briefly are:
\begin{itemize}
    \item There will be less communication between nodes
    \item A Potential for larger communication bandwidth between nodes
    % \item My framework will be able to hold larger models, given the same resources
\end{itemize}

My aims more specifically for this project are to:
\begin{itemize}
    \item Create a prototype RingTMP framework.
    \item Create a parameter server model framework.
    \item Demonstrate less communication between nodes
    \item Demonstrate that RingTMP is at least as scalable than a generic parameter
    server
    \item Demonstrate RingTMP can train neural networks to at least the same
    accuracy  as a generic parameter server
    \item Demonstrate RingTMP can train neural networks in at least the same
    amount of time as a generic parameter server
\end{itemize}

\subsection{Overview}
This document is split up into the following sections:
\begin{itemize}
 \item \textbf{Introduction} Current section. Introduces the project and its aims.
 \item \textbf{Literature Review} Presents related research material in similar applications and areas.
 \item \textbf{Problem Description} Description of the problem aiming to be solved.
 \item \textbf{Implementation} Details of the implementation of the solution.
 \item \textbf{Results} Experimental results and analysis
 \item \textbf{Conclusion} Summary of the project and the paper.
\end{itemize}


% Should do power consumption if I have enough time
% \item Demonstrate that RingTMP takes less overall power to run in comparison
% to the parameter server.



