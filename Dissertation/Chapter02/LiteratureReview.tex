%-----------------------------------------------------------------------------------------
\clearpage
\section{Literature Review}
%-----------------------------------------------------------------------------------------
\subsection{Brief Introduction to Machine Learning and Neural Networks}
To first understand distributed machine learning you must first understand the
fundamentals of machine learning and neural networks. There are many machine
learning methods some requiring training data which we call supervised and some
being able to find patterns in data without being given solutions called
unsupervised. \cite{alpaydin2020introduction} Neural networks tend to focus on
supervised learning and use a form gradient descent called stochastic gradient
descent.

Many machine learning algorithms use a cost function to measure how well or
badly they are solving a problem, these algorithms use parameters which are
internal variables of a machine learning model and define how they solve the
problem. If you map \(costFunction(x)\), where \(x\) is the model parameter, for
every \(x\) value. Then a graph will be produced \(y = costFunction(x)\), the
lowest point on the graph will be the global minimum. There may be other troughs
higher than the global minimum these are called local minimums. A global minimum
represents the lowest value of the cost function which indicates the parameter
values produce the best solution for your problem. Initial model parameters are
often randomised, which likely means they will start at a high point on the cost
function graph, the goal is to get to the lowest point possible. To do this you
must \textit{descend} down the \textit{gradient} to a local minima, the
algorithm that does this is called gradient descent for that very reason. This
often happens in little steps after the observation of each piece of data.
However it is computationally expensive to step down the gradient after each
example. It is more efficient to calculate the average step of a randomised
selection of data. This is know as stochastic gradient descent.

Neural networks are structures that can perform multi-variable gradient descent
when provided with training data. They are comprised of layers of
interconnected neurons in a lattice like structure. Each neuron holds parameter
information the adjusting of which through gradient descent leads to the solving
of a problem through reaching the local minimum of the cost function.

\subsection{Limited History of Distributed Machine Learning}


One of the first instances of distributed machine learning was used to
categorise New York Times articles using Latent Dirichlet Allocation (LDA),
which identifies the affiliations words have to certain topics.
\cite{newman2008distributed} While the paper focused on parallelising the
algorithm and running them over multiple artificially isolated cores the results
showed that distributed machine learning could have scalability and didnâ€™t
impact the rate of convergence of the model significantly. This was followed by
a paper by Jia et al. \cite{ParallelTopicModels} which produced much faster
results than its predecessors by using memcache layer in every machine, every
machine would message every other with updates of its local parameters
to create an approximate global state, it was mentioned in passing that
arranging the nodes in a star topology and caching the values that passed
through it could make the system more scalable. After this followed a cambrian
explosion of work in this area \cite{Ahmed2012YahooLDA, li2014communication,
Dean2012Distbelief, googlemapreduce2008} culminating in 2014 when the parameter
server as it is known today \cite{LI2014ParameterServers} was produced.


\subsection{Model and Data Parallelism}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{DataParallel}
    \includegraphics[width=0.4\textwidth]{ModelParallel}
    \caption{Left: Data Parallelism. Right: Model Parallelism. In both diagrams
        the green lines indicate local parameters being sent to the parameter
        server and red lines indicate parameters being sent to the worker nodes.}
\end{figure}
When creating distributed machine learning models there are two different methods
for distributing training, model parallelism and data parallelism. These two
methods are not mutually exclusive and can be used in conjunction with one
another, such as in DistBelief. \cite{Dean2012Distbelief}. Model parallelism is
when model parameters are split between the nodes. Data parallelism is when
the training data is split between the nodes. \cite{Xing2015Petuum} Often with model
parallelism the whole set of training data is passed through each node. While in
data parallelism its common for each node to hold the whole machine learning
model.

The key advantage of model parallelism is that machine learning models can be
far larger as they no longer have to train the whole model on one machine.
However this one great advantage comes with some disadvantages. Some parameters
may take more time to converge than others, this means that at times some nodes
may be idle while others are still converging, so the spread of computation is
not equal or efficient. \cite{Dean2012Distbelief} Because some parameters
converge at different rates a scheduler can be used, which does improve model
convergence. However this requires more computational overhead and communication
while reducing iteration throughput.
\cite{kim2016STRADS}

Data parallelism has the benefit that data throughput can be very large, making
processing using this method fast. However with more nodes, the
communication overhead increases as the nodes must communicate the changes in
their model parameters to the parameter server. \cite{elgabli2020gadmm}

\subsection{Model Consistency}
Both of these parallel paradigms still function on the basis of a strictly
iterative model, where communication is the limiting factor. Creating a generic
parameter server similar to how Googles map reduce algorithm is implemented
\cite{googlemapreduce2008} is still in some sense sequential. The next iteration
can't continue until all workers have responded with their updated parameters,
and each worker must send its results back no matter how insignificant its work
is. Relaxing these restraints in specific ways have been shown to produce faster
training times, while still converging on a model just as accurate, especially
when time not data is at a premium.\cite{li2014communication}
\par
Partially relaxing the iterative restraint of the parameter server and instead
using a bounded delay, dramatically decreases the wait times of workers to
almost 0. A bounded delay allows workers to operate on slightly stale
parameters. Unintuitively this allows for faster convergence than the sequential
model, as it can iterate over the data faster and learn almost as much from each
iteration. \cite{li2014communication}.
\par
The other restraint that can be relaxed is the requirement to always send
updated weights back to the parameter server. Many training examples don't
dramatically change the parameters. Therefore only the training examples that
cause the parameters to change significantly need to be sent back to the
parameter server. This also allows for greater scalability, as each worker only
communicates significant updates to the parameter server, meaning more nodes can
be used before saturating the network. \cite{li2014communication}. 

\subsection{The Communication Issue}
% TODO: EXPAND by talking about the magnitudes in speed between local training
% and training via nodes? 


Distributed neural networks must communicate with each other in some way in
order to work together. This needs to be formalised to be able to measure the
efficiency of our machine learning system. Parts of this reasoning already
appears in these papers too. \cite{konevcny2016federated ,Ma2017DistributedOptimisation}

If we consider how a neural network operates if we were to run it on a single
node, we could characterise its computation as such:
\begin{equation}
    TIME = I_A (\epsilon) \times T_A 
\end{equation}

Where \(I_A(\epsilon)\) is the number of iterations of the algorithm \(A\) it
takes to reach accuracy \(\epsilon\) and \(T_A\) is the time of each iteration
of the algorithm. Here maximising the convergence per iteration or decreasing
the time an iteration takes will decrease the runtime of the algorithm. In a
distributed setting this equation changes to this:
\begin{equation}
    TIME = I_A (\epsilon) \times (c + T_A)  
\end{equation}
In this equation we have the added variable \(c\), this represents time taken
for communication per iteration. In a distributed setting this will always
remain above a non trivial amount of data. Unfortunately the majority of machine
learning algorithms use a stochastic method which means a very large number of
iterations \((I_A(\epsilon))\) that take a short amount of time (small \(T_A\)).
You can see that no matter how small \(c\) is there will be a significant impact
of the time taken. In fact with a naive approach of communication \(c > T_A\) is
almost certainly true for each iteration.

However this view doesn't take into account the possibility that communication
could happen at the same time as an iteration. For example imagine a pipeline of
nodes where each nodes performs an iteration but can communicate its previous
iteration to the next node at the same time. Then the time taken could be
described like so:
\begin{equation}
    TIME = I_A (\epsilon) \times max(c, T_A)
\end{equation}
Here you can see that if you can find a way of making the communication time
equal to the time per iteration. Then \(c\) would have a negligible effect on
the equation.


% Do I need to include this?
\subsection{Low Power Hardware, IoT and Mobile Computation}

Historically machine learning algorithms have been focused on high model
accuracy through large models and vast amounts of training data, energy
consumption and efficiency has rarely been taken into consideration. However
with the rise of Internet of Things (IoT) devices and the established ubiquity
of smart phones more data than ever is being produced. Soon this data generation
will exceed the capacity of the internet, and experts estimate that over 90\% of
data will be stored and processed locally. \cite{Chaing2016FogIoT} By extension
this means machine learning algorithms will have to be performed locally too.
This introduces some challenging issues. Modern machine learning algorithms
require vast centralised computational power and large amounts of data. Local
devices don't have the capacity to hold large data sets or the power to compute
large machine learning models in a viable amount of time, while many of them are
also battery powered so power consumption becomes another issue.

% modern solutions for edge computing 
A solution to this is to massively distribute the model over multiple
decentralised nodes. The level of distribution is even greater than that of
centralised compute clusters. In this solution each device computes a model
using its own local data, infrequently (due to network constraints) the model is
shared with a coordination server, which will then distribute the changes across
all nodes in the network. \cite{wang2018EdgeLearning} While this method is
inefficient as the infrequent communications mean that many nodes may do much of
the same work, and the merging of local models into a global one infrequently
may cause loss of information. It still produces a model which converges in a
relatively few rounds of communication. \cite{konevcny2016federated}

% low power solutions
Efforts have been made in techniques to reduce the power, memory and storage
needed for machine learning algorithms to operate. One of these is data stream
mining. The idea is that a device can stream the analytics data directly into
the model rather than storing the data for later use.
\cite{garciaMartin2019MLEnergy} This means after the data has been read by the
model it is lost. But that also means that no data needs to be stored, meaning
resources are not spent reading and writing to storage. This has an application
in mobile devices, as they produce data at a low rate through user interaction.
Therefore the model can be build in real time as actions occur. The data
produced on the mobile itself may not be enough to effectively train the model,
but via communication with other users distributed over the network the model
can converge. \cite{konevcny2016federated}

