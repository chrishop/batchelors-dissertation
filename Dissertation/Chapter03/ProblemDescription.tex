%-----------------------------------------------------------------------------------------
\clearpage
\section{Project Plan and Time Management}
%-----------------------------------------------------------------------------------------


\subsection{Problem Description}

% show the ideal situation
In the most ideal world, communication between distributed network nodes would
never saturate bandwidth and each node would spend all of its time on
computation rather than waiting. This would enable distributed machine learning
to iterate just as fast as a single node machine learning.

% show the situation as it currently exists
Ultimately we want to make neural networks train faster. The way this is
achieved from the perspective of distributed neural networks is to create more
neural network nodes. The communication between nodes often saturates the
network, when this happens adding more nodes no longer speeds up computation. To
increase the speed of training further at least one of two things must happen.
Either the bandwidth between each node must be increased or the nodes must
communicate less while still communicating enough information to continue training.
% add some more evidence with citation here
% less communication bandwidth you can cite significant update thing, or compressing
% carnigie mellon university did some work with globe distributed machine learning, look that up


% As it stands bandwidth is the factor limiting how many
% nodes a distributed neural network can scale to. This in turn 

% the consequences of not solving the problem


% Bandwidth restrictions between nodes place a limit
% Through reading the relevant literature, the key problem of this project is this. How can we optimise communication between workers



