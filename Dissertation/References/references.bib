@inproceedings{patel2011joomla,
  title={Joomla, Drupal and WordPress-a statistical comparison of open source CMS},
  author={Patel, Savan K and Rathod, VR and Parikh, Satyen},
  booktitle={Trendz in Information Sciences and Computing (TISC), 2011 3rd International Conference on},
  pages={182--187},
  year={2011},
  organization={IEEE}
},
@misc{
  mattBurgessFacialRecognision,
  title={Some UK Stores Are Using Facial Recognition to Track Shoppers},
  url={https://www.wired.com/story/uk-stores-facial-recognition-track-shoppers/},
  journal={Wired},
  publisher={Conde Nast},
  author={Matt Burgess, WIRED UK}
},
@Article{Mammograms2020,
author={McKinney, Scott Mayer
and Sieniek, Marcin
and Godbole, Varun
and Godwin, Jonathan
and Antropova, Natasha
and Ashrafian, Hutan
and Back, Trevor
and Chesus, Mary
and Corrado, Greg S.
and Darzi, Ara
and Etemadi, Mozziyar
and Garcia-Vicente, Florencia
and Gilbert, Fiona J.
and Halling-Brown, Mark
and Hassabis, Demis
and Jansen, Sunny
and Karthikesalingam, Alan
and Kelly, Christopher J.
and King, Dominic
and Ledsam, Joseph R.
and Melnick, David
and Mostofi, Hormuz
and Peng, Lily
and Reicher, Joshua Jay
and Romera-Paredes, Bernardino
and Sidebottom, Richard
and Suleyman, Mustafa
and Tse, Daniel
and Young, Kenneth C.
and De Fauw, Jeffrey
and Shetty, Shravya},
title={International evaluation of an AI system for breast cancer screening},
journal={Nature},
year={2020},
month={Jan},
day={01},
volume={577},
number={7788},
pages={89-94},
abstract={Screening mammography aims to identify breast cancer at earlier stages of the disease, when treatment can be more successful1. Despite the existence of screening programmes worldwide, the interpretation of mammograms is affected by high rates of false positives and false negatives2. Here we present an artificial intelligence (AI) system that is capable of surpassing human experts in breast cancer prediction. To assess its performance in the clinical setting, we curated a large representative dataset from the UK and a large enriched dataset from the USA. We show an absolute reduction of 5.7{\%} and 1.2{\%} (USA and UK) in false positives and 9.4{\%} and 2.7{\%} in false negatives. We provide evidence of the ability of the system to generalize from the UK to the USA. In an independent study of six radiologists, the AI system outperformed all of the human readers: the area under the receiver operating characteristic curve (AUC-ROC) for the AI system was greater than the AUC-ROC for the average radiologist by an absolute margin of 11.5{\%}. We ran a simulation in which the AI system participated in the double-reading process that is used in the UK, and found that the AI system maintained non-inferior performance and reduced the workload of the second reader by 88{\%}. This robust assessment of the AI system paves the way for clinical trials to improve the accuracy and efficiency of breast cancer screening.},
issn={1476-4687},
doi={10.1038/s41586-019-1799-6},
url={https://doi.org/10.1038/s41586-019-1799-6}
},
@misc{piergiovanni2020avid,
      title={AViD Dataset: Anonymized Videos from Diverse Countries}, 
      author={AJ Piergiovanni and Michael S. Ryoo},
      year={2020},
      eprint={2007.05515},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
},
@misc{fewshowlearners2020gpt,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
},
@misc{rajbhandari2020zero,
      title={ZeRO: Memory Optimizations Toward Training Trillion Parameter Models}, 
      author={Samyam Rajbhandari and Jeff Rasley and Olatunji Ruwase and Yuxiong He},
      year={2020},
      eprint={1910.02054},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
},
@inproceedings {LI2014ParameterServers,
author = {Mu Li and David G. Andersen and Jun Woo Park and Alexander J. Smola and Amr Ahmed and Vanja Josifovski and James Long and Eugene J. Shekita and Bor-Yiing Su},
title = {Scaling Distributed Machine Learning with the Parameter Server},
booktitle = {11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14)},
year = {2014},
isbn = { 978-1-931971-16-4},
address = {Broomfield, CO},
pages = {583--598},
url = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/li_mu},
publisher = {{USENIX} Association},
month = oct,
},
@misc{jia2018BeyondData,
      title={Beyond Data and Model Parallelism for Deep Neural Networks}, 
      author={Zhihao Jia and Matei Zaharia and Alex Aiken},
      year={2018},
      eprint={1807.05358},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
},
@incollection{Dean2012Distbelief,
title = {Large Scale Distributed Deep Networks},
author = {Jeffrey Dean and Greg Corrado and Rajat Monga and Chen, Kai and Matthieu Devin and Mark Mao and Marc\textquotesingle aurelio Ranzato and Andrew Senior and Paul Tucker and Ke Yang and Quoc V. Le and Andrew Y. Ng},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1223--1231},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf}
},
@inproceedings{kim2016STRADS,
author = {Kim, Jin Kyu and Ho, Qirong and Lee, Seunghak and Zheng, Xun and Dai, Wei and Gibson, Garth A. and Xing, Eric P.},
title = {STRADS: A Distributed Framework for Scheduled Model Parallel Machine Learning},
year = {2016},
isbn = {9781450342407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901318.2901331},
doi = {10.1145/2901318.2901331},
abstract = {Machine learning (ML) algorithms are commonly applied to big data, using distributed systems that partition the data across machines and allow each machine to read and update all ML model parameters --- a strategy known as data parallelism. An alternative and complimentary strategy, model parallelism, partitions the model parameters for non-shared parallel access and updates, and may periodically repartition the parameters to facilitate communication. Model parallelism is motivated by two challenges that data-parallelism does not usually address: (1) parameters may be dependent, thus naive concurrent updates can introduce errors that slow convergence or even cause algorithm failure; (2) model parameters converge at different rates, thus a small subset of parameters can bottleneck ML algorithm completion. We propose scheduled model parallelism (SchMP), a programming approach that improves ML algorithm convergence speed by efficiently scheduling parameter updates, taking into account parameter dependencies and uneven convergence. To support SchMP at scale, we develop a distributed framework STRADS which optimizes the throughput of SchMP programs, and benchmark four common ML applications written as SchMP programs: LDA topic modeling, matrix factorization, sparse least-squares (Lasso) regression and sparse logistic regression. By improving ML progress per iteration through SchMP programming whilst improving iteration throughput through STRADS we show that SchMP programs running on STRADS outperform non-model-parallel ML implementations: for example, SchMP LDA and SchMP Lasso respectively achieve 10x and 5x faster convergence than recent, well-established baselines.},
booktitle = {Proceedings of the Eleventh European Conference on Computer Systems},
articleno = {5},
numpages = {16},
location = {London, United Kingdom},
series = {EuroSys '16}
}
