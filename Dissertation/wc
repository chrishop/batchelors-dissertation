In the most ideal world, communication between distributed network nodes would
never saturate bandwidth and each node would spend all of its time on
computation rather than waiting. This would enable distributed machine learning
to iterate just as fast as a single node machine learning.

% show the situation as it currently exists
Ultimately we want to make neural networks train faster. The way this is
achieved from the perspective of distributed neural networks is to create more
neural network nodes. The communication between nodes often saturates the
network, when this happens adding more nodes no longer speeds up computation. To
increase the speed of training further at least one of two things must happen.
Either the bandwidth between each node must be increased or the nodes must
communicate less while still communicating enough information to continue training.