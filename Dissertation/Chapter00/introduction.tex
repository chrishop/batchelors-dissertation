%-----------------------------------------------------------------------------------------

\section{Introduction}
%-----------------------------------------------------------------------------------------

\subsection{Motivation and Context}

Machine Learning has become an invisible but ubiquitous part of modern life, and
is being used in a plethora of fields and industries. The uses of this
technology range from dystopian facial recognition
\cite{mattBurgessFacialRecognision} to lifesaving diagnoses
\cite{Mammograms2020} and many more purposes besides. Machine Learning leverages
existing data to train Machine Learning models in order to perform a task or
find patterns, that previously only a human could. The key difference between
Machine Learning and conventional programs is that the data itself is used to
develop the model. Therefore the quality and quantity of the data can affect the
effectiveness of a machine learning model.
\par
As the amount and complexity of the data we are collecting increases so does the
size and complexity of the machine learning models we use to make sense of our
data. For example the internet archive as of 2020 contains over 70 petabytes of
data, while labeled datasets such as AViD have video data in the order of
terabytes. \cite{piergiovanni2020avid} We are now reaching a point where the
limiting factor of creating a machine learning model is not the data, but the
machine learning algorithm itself.
\par
This problem is two pronged. First machine learning models are getting very,
very large. For instance GPT-3 the largest Natural Language Model ever trained
contains 175 billion parameters. \cite{fewshowlearners2020gpt} And efforts are
being made to create models with trillions of parameters.
\cite{rajbhandari2020zero} We have reached the point where its no longer
possible to store some machine learning models on a single
machine.\cite{LI2014ParameterServers}
The second problem is that training a machine learning model is increasingly
taking longer and longer. This is because we have more data and larger models,
but the algorithms used to train models have fundamentally stayed the same and
are inherently sequential and difficult to parallelise.
\par
The popular current solution is to use a parameter server model. In brief the
paradigm is made of two different types of components. The parameter server and
the workers. The parameter server holds the global parameters of the model.
Workers are given the model parameters by the parameter server. The workers then
perform an iteration of whichever machine learning algorithm they are
performing, modifying the parameters. Then the modified parameters are sent to
the parameter server where they are aggregated, the global model parameters are
updated and the cycle continues until the model has converged on an answer.
However this method has two key drawbacks. First, every worker must communicate
with a single parameter server, this limits scalability as eventually the
network bandwidth becomes saturated severely impacting performance.
\cite{LI2014ParameterServers} Secondly many parameter server models require the
whole model to be replicated within each node. \cite{jia2018BeyondData} This
means that very large models simply cannot run on many machines.
\par
In this paper I will outline an alternative distributed machine learning
framework: \textit{RingTMP}. RingTMP (Ring Topological Model Parallel) is a Ring
Topological Model Parallel distributed machine learning framework focusing on
optimising Distributed Gradient Descent. This is a novel design drawing in
inspiration much research but particularly from the STRADS and DistBelief
machine learning frameworks. \cite{kim2016STRADS,Dean2012Distbelief}
\par
I believe my distributed framework may have some advantages over the current
paradigm, these briefly are:
\begin{itemize}
    \item There will be less communication between nodes
    \item A Potential for larger communication bandwidth between nodes
    \item My framework will be able to hold larger models
    \item Will be able to train neural networking models as faster or faster
    than a comparative parameter server, to the same level of accuracy
\end{itemize}

My aims more specifically for this project are to:
\begin{itemize}
    \item Create a prototype RingTMP framework.
    \item Create a parameter server model framework.
    \item Demonstrate less communication between nodes
    \item Demonstrate that RingTMP is at least as scalable than a generic parameter
    server.
    \item Demonstrate that RingTMP can hold larger models on comparison to a
    standard parameter server.
    \item Demonstrate RingTMP can train Neural Networks to at least the same
    accuracy in at least the same amount of time.
\end{itemize}


% Should do power consumption if I have enough time
% \item Demonstrate that RingTMP takes less overall power to run in comparison
% to the parameter server.



